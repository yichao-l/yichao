{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b877ea39",
      "metadata": {
        "id": "b877ea39"
      },
      "source": [
        "# Homework 9"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bb5082b",
      "metadata": {
        "id": "1bb5082b"
      },
      "source": [
        "## Imports and Utilities\n",
        "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9a5684c4",
      "metadata": {
        "id": "9a5684c4"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from math import sqrt, log\n",
        "import abc\n",
        "import numpy as np\n",
        "import functools\n",
        "\n",
        "\n",
        "class MDP:\n",
        "    \"\"\"A Markov Decision Process.\"\"\"\n",
        "\n",
        "    @property\n",
        "    @abc.abstractmethod\n",
        "    def state_space(self):\n",
        "        \"\"\"Representation of the MDP state set.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "    @property\n",
        "    @abc.abstractmethod\n",
        "    def action_space(self):\n",
        "        \"\"\"Representation of the MDP action set.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "    @property\n",
        "    def temporal_discount_factor(self):\n",
        "        \"\"\"Gamma, defaults to 1.\n",
        "        \"\"\"\n",
        "        return 1.\n",
        "\n",
        "    @property\n",
        "    def horizon(self):\n",
        "        \"\"\"H, defaults to inf.\n",
        "        \"\"\"\n",
        "        return float(\"inf\")\n",
        "\n",
        "    def state_is_terminal(self, state):\n",
        "        \"\"\"Designate certain states as terminal (done) states.\n",
        "\n",
        "        Defaults to False.\n",
        "\n",
        "        Args:\n",
        "            state: A state.\n",
        "\n",
        "        Returns:\n",
        "            state_is_terminal : A bool.\n",
        "        \"\"\"\n",
        "        return False\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def get_reward(self, state, action, next_state=None):\n",
        "        \"\"\"Return (deterministic) reward for executing action\n",
        "        in state.\n",
        "\n",
        "        Args:\n",
        "            state: A current state.\n",
        "            action: An action.\n",
        "            next_state: Optional. A next state.\n",
        "\n",
        "        Returns:\n",
        "            reward : Single time step reward.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def get_transition_distribution(self, state, action):\n",
        "        \"\"\"Return a distribution over next states.\n",
        "\n",
        "        The form of this distribution will vary, e.g., depending\n",
        "        on whether the MDP has discrete or continuous states.\n",
        "\n",
        "        Args:\n",
        "            state: A current state.\n",
        "            action: An action.\n",
        "\n",
        "        Returns:\n",
        "            next_state_distribution: Distribution over next states.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "    def sample_next_state(self, state, action, rng=np.random):\n",
        "        \"\"\"Sample a next state from the transition distribution.\n",
        "\n",
        "        This function may be overwritten by subclasses when the explicit\n",
        "        distribution is too large to enumerate.\n",
        "\n",
        "        Args:\n",
        "            state: A state from the state space.\n",
        "            action: An action from the action space.\n",
        "            rng: A random number generator.\n",
        "\n",
        "        Returns:\n",
        "            next_state: A sampled next state from the state space.\n",
        "        \"\"\"\n",
        "        next_state_dist = self.get_transition_distribution(state, action)\n",
        "        next_states, probs = zip(*next_state_dist.items())\n",
        "        next_state_index = rng.choice(len(next_states), p=probs)\n",
        "        next_state = next_states[next_state_index]\n",
        "        return next_state\n",
        "\n",
        "\n",
        "class POMDP(MDP):\n",
        "    \"\"\"A partially observable Markov decision process (POMDP).\"\"\"\n",
        "\n",
        "    @property\n",
        "    @abc.abstractmethod\n",
        "    def observation_space(self):\n",
        "        \"\"\"Representation of the POMDP observation space.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "    @abc.abstractclassmethod\n",
        "    def get_observation_distribution(self, next_state, action):\n",
        "        \"\"\"Return a distribution over the observations.\n",
        "\n",
        "        The form of this distribution will vary, e.g., depending\n",
        "        on whether the MDP has discrete or continuous observation\n",
        "        spaces.\n",
        "\n",
        "        Args:\n",
        "            next_state: The next state.\n",
        "            action: The action taken.\n",
        "\n",
        "        Returns:\n",
        "            observation_distribution: Distribution over the observation.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Override me\")\n",
        "\n",
        "\n",
        "class LambdaMDP(MDP):\n",
        "    \"\"\"A helper class that creates a MDP class based on a set of functions.\n",
        "    See the constructor for details.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_space, action_space, state_is_terminal_fn, get_reward_fn, get_transition_distribution_fn, temporal_discount_factor=1.0):\n",
        "        \"\"\"\n",
        "        Construct a MDP class based on a set of function definitions.\n",
        "\n",
        "        Args:\n",
        "            state_space: The set of possible states.\n",
        "            action_space: The set of possible actions.\n",
        "            state_is_terminal_fn: A callable function: state_is_terminal_fn(state) -> bool,\n",
        "                mapping a state to a boolean value indicating whether\n",
        "                the state is a terminal state.\n",
        "            get_reward_fn: A callable function: get_reward_fn(state, action, next_state) -> float,\n",
        "                mapping a (s, a, s') tuple to a float reward value.\n",
        "            get_transition_distribution_fn: A callable function:\n",
        "                get_transition_distribution_fn(state, action) -> distribution of the next state.\n",
        "                Note that the return value for this function must be a discrete distribution.\n",
        "            temporal_discount_factor: A float number, the temporal discount factor of the MDP.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.state_space_v = state_space\n",
        "        self.action_space_v = action_space\n",
        "        self.state_is_terminal = state_is_terminal_fn\n",
        "        self.get_reward = get_reward_fn\n",
        "        self.get_transition_distribution = get_transition_distribution_fn\n",
        "        self.temporal_discount_factor_v = temporal_discount_factor\n",
        "\n",
        "    @property\n",
        "    def state_space(self):\n",
        "        return self.state_space_v\n",
        "\n",
        "    @property\n",
        "    def action_space(self):\n",
        "        return self.action_space_v\n",
        "\n",
        "    @property\n",
        "    def temporal_discount_factor(self):\n",
        "        return self.temporal_discount_factor_v\n",
        "\n",
        "\n",
        "class DiscreteDistribution(object):\n",
        "    \"\"\"A discrete distribution, represneted as a dictionary.\"\"\"\n",
        "\n",
        "    eps = 1e-6\n",
        "\n",
        "    def __init__(self, prob_dict):\n",
        "        \"\"\"Construct a discrete distribution based on a probability dictionary.\n",
        "        The dictionary might be \"sparse\", in which case the omitted entries are\n",
        "        treated as zero-probability values.\n",
        "\n",
        "        Note that, even if the random varaible takes values from a continuous space,\n",
        "        (e.g., all real numbers), we can still define a \"discrete distribution\",\n",
        "        that is, a distribution only has mass on a finite set of points.\n",
        "        For example, we can define a distribution on R: {0: 0.5, 1: 0.5}.\n",
        "        Implicitly, all values not in the prob_dict will be treated as\n",
        "        zero-probability.\n",
        "\n",
        "        Example:\n",
        "\n",
        "        ```\n",
        "        p = DiscreteDistribution({'x': 0.0, 'y': 0.6, 'z': 0.4})\n",
        "        print(p.p('x'))  # 0.0\n",
        "        for x in p:  # iterate over the set of possible values.\n",
        "            print(x, p.p(x))  # should print y 0.6 z 0.4\n",
        "        for x, p_x in p.items():  # just like iterating over a Python dict.\n",
        "            print(x, p_x)  # should print y 0.6 z 0.4\n",
        "        ```\n",
        "        Note that, during iteration, zero-probability values will be omitted.\n",
        "\n",
        "        Args:\n",
        "            prob_dict: A dictionary, mapping elements in the domain to a float\n",
        "                number. The dictionary might be sparse. It should always\n",
        "                sum up to one (thus being a valid distribution.)\n",
        "        \"\"\"\n",
        "        self.prob_dict = prob_dict\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Iterate over the support set.\"\"\"\n",
        "        yield from self.support()\n",
        "\n",
        "    def support(self):\n",
        "        \"\"\"Itearte over the support set of the distribution. That is,\n",
        "        values with a non-zero probability mass.\n",
        "        \"\"\"\n",
        "        for k, v in self.prob_dict.items():\n",
        "            if v > 0:\n",
        "                yield k\n",
        "\n",
        "    def items(self):\n",
        "        \"\"\"Iterate over the distribution. Generates a list of (x, p(x)) pairs.\n",
        "        This function will ignore zero-probability values in the prob_dict.\n",
        "        \"\"\"\n",
        "        for k, v in self.prob_dict.items():\n",
        "            if v > 0:\n",
        "                yield k, v\n",
        "\n",
        "    def p(self, value):\n",
        "        \"\"\"Evaluate the proabbility of a value in the support set.\n",
        "\n",
        "        Args:\n",
        "            value: An object in the domain of the distribution.\n",
        "\n",
        "        Returns:\n",
        "            p: A float, indicating p(value). For values not in the support (prob_dict),\n",
        "                the probability is assumed to be zero.\n",
        "        \"\"\"\n",
        "        return self.prob_dict.get(value, 0.)\n",
        "\n",
        "    def renormalize(self):\n",
        "        \"\"\"Renormalize the distribution to ensure that the probabilities sum up to 1.\n",
        "\n",
        "        Returns:\n",
        "            self\n",
        "        \"\"\"\n",
        "        z = sum(self.prob_dict.values())\n",
        "        assert z > 0, 'Degenerated probability distribution.'\n",
        "        self.prob_dict = {k: v / z for k, v in self.prob_dict.items()}\n",
        "        return self\n",
        "\n",
        "    def check_normalization(self):\n",
        "        \"\"\"Check if the prob dict is correctly normalized (i.e., should sum up to 1).\"\"\"\n",
        "        assert 1 - type(self).eps < sum(self.prob_dict.values()) < 1 + type(self).eps\n",
        "\n",
        "    def max(self):\n",
        "        \"\"\"Return argmax_x p(x).\n",
        "\n",
        "        Returns:\n",
        "            arg_max: An object in the support, argmax_x p(x).\n",
        "        \"\"\"\n",
        "        return max(self.prob_dict, key=lambda x: (self.prob_dict[x], x))\n",
        "\n",
        "    def draw(self, rng=None):\n",
        "        if rng is None:\n",
        "            rng = np.random\n",
        "        keys = list(self.prob_dict.keys())\n",
        "        probs = [self.prob_dict[k] for k in keys]\n",
        "        return keys[rng.choice(len(keys), p=probs)]\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.prob_dict)\n",
        "\n",
        "    def as_tuple(self):\n",
        "        return tuple((self.prob_dict.get(k), k) for k in sorted(self.support()))\n",
        "\n",
        "    def __lt__(self, other):\n",
        "        return self.as_tuple() < other.as_tuple()\n",
        "\n",
        "    def __gt__(self, other):\n",
        "        return self.as_tuple() > other.as_tuple()\n",
        "\n",
        "\n",
        "def OnehotDiscreteDistribution(obj):\n",
        "    \"\"\"Create a DiscreteDistribution of p(obj) = 1.\"\"\"\n",
        "    return DiscreteDistribution({obj: 1.0})\n",
        "\n",
        "\n",
        "def UniformDiscreteDistribution(support):\n",
        "    \"\"\"Create a DiscreteDistribution that is uniform. That is, for any object x, p(x) = 1 / |support|.\"\"\"\n",
        "    return DiscreteDistribution({x: 1 / len(support) for x in support})\n",
        "\n",
        "# Our RobotChargingPOMDP\n",
        "\n",
        "class RobotChargingPOMDP(POMDP):\n",
        "    DEF_MOVE_SUCCESS = 0.8\n",
        "    DEF_OBS_IF_THERE = 0.9\n",
        "    DEF_OBS_IF_NOT_THERE = 0.4\n",
        "    DEF_C_MOVE = 0.5\n",
        "    DEF_C_LOOK = 0.1\n",
        "    DEF_GAMMA = 0.9\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        p_move_success=DEF_MOVE_SUCCESS, p_obs_if_there=DEF_OBS_IF_THERE, p_obs_if_not_there=DEF_OBS_IF_NOT_THERE,\n",
        "        c_move=DEF_C_MOVE, c_look=DEF_C_LOOK,\n",
        "        gamma=DEF_GAMMA\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create the Robot Charging POMDP.\n",
        "\n",
        "        Args:\n",
        "            p_move_success (float): the probability that a move action is successful.\n",
        "            p_obs_if_there (float): the probability of return 1 when looking at a location with the charger.\n",
        "            p_obs_if_not_there (float): the probability of return 1 when looking at a location without a charger.\n",
        "            c_move (float): the cost of a move action.\n",
        "            c_look (float): the cost of a look action.\n",
        "            gamma (float): the temporal discount factor.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.p_move_success = p_move_success\n",
        "        self.p_obs_if_there = p_obs_if_there\n",
        "        self.p_obs_if_not_there = p_obs_if_not_there\n",
        "        self.c_move = c_move\n",
        "        self.c_look = c_look\n",
        "        self.gamma = gamma\n",
        "\n",
        "    @property\n",
        "    def state_space(self):\n",
        "        \"\"\"\n",
        "        Three \"normal\" states: 0, 1, 2, indicating the position of the charger.\n",
        "        One \"terminal\" state T. Executing the \"charge\" action will reach this\n",
        "        terminal state. And the state is absorbing. The robot will deterministically\n",
        "        transition to this terminal state when we execute the c action.\n",
        "        \"\"\"\n",
        "        return {0, 1, 2, 'T'}\n",
        "\n",
        "    @property\n",
        "    def action_space(self):\n",
        "        # lx: look(x)\n",
        "        # mxy: move(start=x, target=y)\n",
        "        # c: charge\n",
        "        # nop: NOP\n",
        "        return {'l0', 'l1', 'l2', 'm01', 'm12', 'm20', 'c', 'nop'}\n",
        "\n",
        "    @property\n",
        "    def observation_space(self):\n",
        "        return {0, 1}\n",
        "\n",
        "    @property\n",
        "    def temporal_discount_factor(self):\n",
        "        return self.gamma\n",
        "\n",
        "    def state_is_terminal(self, state):\n",
        "        return state == 'T'\n",
        "\n",
        "    def get_reward(self, state, action, next_state=None):\n",
        "        if action == 'nop':\n",
        "            return 0\n",
        "        elif action == 'c':\n",
        "            if state == 0:\n",
        "                return 10\n",
        "            else:\n",
        "                return -100\n",
        "        elif action.startswith('m'):\n",
        "            return -self.c_move\n",
        "        else:  # look\n",
        "            return -self.c_look\n",
        "\n",
        "    def get_transition_distribution(self, state, action):\n",
        "        if action == 'c':\n",
        "            return OnehotDiscreteDistribution('T')\n",
        "        elif action.startswith('m'):\n",
        "            start, target = int(action[1]), int(action[2])\n",
        "            if state == start:\n",
        "                return DiscreteDistribution({target : self.p_move_success, start : 1 - self.p_move_success})\n",
        "        return OnehotDiscreteDistribution(state)\n",
        "\n",
        "    def get_observation_distribution(self, next_state, action):\n",
        "        if action.startswith('l'):\n",
        "            target = int(action[1])\n",
        "            if next_state == target:\n",
        "                return DiscreteDistribution({0: 1 - self.p_obs_if_there, 1: self.p_obs_if_there})\n",
        "            else:\n",
        "                return DiscreteDistribution({0: 1 - self.p_obs_if_not_there, 1: self.p_obs_if_not_there})\n",
        "        return OnehotDiscreteDistribution(0)\n",
        "\n",
        "\n",
        "\n",
        "def bellman_backup(s, V, mdp):\n",
        "    \"\"\"Look ahead one step and propose an update for the value of s.\n",
        "\n",
        "    You can assume that the mdp is either infinite or indefinite\n",
        "    horizon (that is, mdp.horizon is inf).\n",
        "\n",
        "    Args:\n",
        "        s: A state.\n",
        "        V: A dict, V[state] -> value.\n",
        "        mdp: An MDP.\n",
        "\n",
        "    Returns:\n",
        "        vs: new value estimate for s.\n",
        "    \"\"\"\n",
        "\n",
        "    assert mdp.horizon == float(\"inf\")\n",
        "    vs = -float(\"inf\")\n",
        "    for a in mdp.action_space:\n",
        "        qsa = 0.\n",
        "        for ns, p in mdp.get_transition_distribution(s, a).items():\n",
        "            r = mdp.get_reward(s, a, ns)\n",
        "            qsa += p * (r + mdp.temporal_discount_factor * V[ns])\n",
        "        vs = max(qsa, vs)\n",
        "    return vs\n",
        "\n",
        "\n",
        "def value_iteration(mdp, max_num_iters=1000, change_threshold=1e-4):\n",
        "    \"\"\"Run value iteration for a certain number of iterations or until\n",
        "    the max change between iterations is below a threshold.\n",
        "\n",
        "    You can assume that the mdp is either infinite or indefinite\n",
        "    horizon (that is, mdp.horizon is inf).\n",
        "\n",
        "    Args:\n",
        "        mdp: An MDP.\n",
        "        max_num_iters: An int representing the maximum number of\n",
        "        iterations to run value iteration before giving up.\n",
        "        change_threshold: A float used to determine when value iteration\n",
        "        has converged and it is safe to terminate.\n",
        "\n",
        "    Returns:\n",
        "        V:  A dict, V[state] -> value.\n",
        "        it: The number of iterations before convergence.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize V to all zeros\n",
        "    V = {s: 0. for s in mdp.state_space}\n",
        "\n",
        "    for it in range(max_num_iters):\n",
        "        next_V = {}\n",
        "        max_change = 0.\n",
        "        for s in mdp.state_space:\n",
        "            if mdp.state_is_terminal(s):\n",
        "                next_V[s] = 0.\n",
        "            else:\n",
        "                next_V[s] = bellman_backup(s, V, mdp)\n",
        "            max_change = max(abs(next_V[s] - V[s]), max_change)\n",
        "        V = next_V\n",
        "        if max_change < change_threshold:\n",
        "            break\n",
        "    return V, it\n",
        "\n",
        "\n",
        "def qsa_from_vs(mdp, V):\n",
        "    \"\"\"Compute Q(s, a) based on V(s).\n",
        "\n",
        "    Args:\n",
        "        mdp: An MDP.\n",
        "        V: A dict, V[state] -> value. Typically, this is computed by value_iteration.\n",
        "\n",
        "    Returns:\n",
        "        Q: A dict, Q[state, action] -> value.\n",
        "    \"\"\"\n",
        "\n",
        "    Q = dict()\n",
        "    for s in mdp.state_space:\n",
        "        if not mdp.state_is_terminal(s):\n",
        "            for a in mdp.action_space:\n",
        "                qsa = 0.\n",
        "                for ns, p in mdp.get_transition_distribution(s, a).items():\n",
        "                    r = mdp.get_reward(s, a, ns)\n",
        "                    qsa += p * (r + mdp.temporal_discount_factor * V[ns])\n",
        "                Q[s, a] = qsa\n",
        "        else:\n",
        "            for a in mdp.action_space:\n",
        "                Q[s, a] = 0\n",
        "    return Q\n",
        "\n",
        "\n",
        "def expectimax_search(initial_state, mdp, horizon, return_Q=False):\n",
        "    \"\"\"Use expectimax search to determine a next action.\n",
        "\n",
        "    Note that we're just computing the single next action to\n",
        "    take, we do not need to store the entire partial V.\n",
        "\n",
        "    Horizon is given as a separate argument so that we can use\n",
        "    expectimax search with receding horizon control, for example,\n",
        "    even if mdp.horizon is inf.\n",
        "\n",
        "    Args:\n",
        "        initial_state: A state in the mdp.\n",
        "        mdp (MDP): An MDP.\n",
        "        horizon (int): An int horizon.\n",
        "        return_Q (bool): A boolean value. If true, also return the Q value\n",
        "            at the root instead of the action.\n",
        "\n",
        "    Returns:\n",
        "        action: An action in the mdp.\n",
        "        Q: The Q value at the root state (only when return_Q is True).\n",
        "    \"\"\"\n",
        "    A = sorted(mdp.action_space)\n",
        "    R = mdp.get_reward\n",
        "    P = mdp.get_transition_distribution\n",
        "    gm = mdp.temporal_discount_factor\n",
        "    ts = mdp.state_is_terminal\n",
        "\n",
        "    # Cache the V(s, h)'s that have been computed.\n",
        "    def V(s, h):\n",
        "        if h == horizon or ts(s):\n",
        "            return 0\n",
        "        return max(Q(s, a, h) for a in A)\n",
        "\n",
        "    def Q(s, a, h):\n",
        "        psa = P(s, a)\n",
        "        # psa is a DiscreteDistribution over beliefs.  ns is a belief.\n",
        "        return sum(psa.p(ns) * (R(s, a, ns) + gm * V(ns, h+1)) for ns in psa)\n",
        "\n",
        "    Q_values = {a: Q(initial_state, a, 0) for a in A}\n",
        "    if return_Q:\n",
        "        return max(A, key=Q_values.get), Q_values\n",
        "    return max(A, key=Q_values.get)\n",
        "\n",
        "\n",
        "def transition_update(pomdp, belief, action):\n",
        "    \"\"\"Compute p(s') from a prior distribution of p(s) based on the transition\n",
        "    distribution p(s, action, s').\n",
        "\n",
        "    Args:\n",
        "        pomdp (POMDP): A POMDP object.\n",
        "        belief (DiscreteDistribution): A distribution over the current state s.\n",
        "        action: The action to be executed.\n",
        "\n",
        "    Returns:\n",
        "        updated_belief (DiscreteDistribution): A distribution over the next state s'.\n",
        "    \"\"\"\n",
        "    updated = {x: 0.0 for x in pomdp.state_space}\n",
        "    for s in belief:\n",
        "        prob_s = belief.p(s)\n",
        "        for s_prime, transition_prob in pomdp.get_transition_distribution(s, action).items():\n",
        "            updated[s_prime] += transition_prob * prob_s\n",
        "    # Note that we don't necessarily need to renormalize the distribution (it is self-normalized!)\n",
        "    # The added `.renormalize()` part is really just for numeric stability.\n",
        "    return DiscreteDistribution(updated).renormalize()\n",
        "\n",
        "\n",
        "def observation_update(pomdp, belief, action, observation):\n",
        "    \"\"\"Compute p(s' | observation, action) following the Bayes rule.\n",
        "        p(s' | o, a) is proportional to p(s' | a) * p(o | s', a).\n",
        "\n",
        "    Args:\n",
        "        pomdp (POMDP): A POMDP object.\n",
        "        belief (DiscreteDistribution): The distribution over the next state: p(s' | a).\n",
        "            Typically, this is the output of the transition_update() function.\n",
        "        action: The action taken.\n",
        "        observation: The observation.\n",
        "\n",
        "    Returns:\n",
        "        posterior (DiscreteDistribution): The updated belief over the next state s'.\n",
        "            Normalized!\n",
        "    \"\"\"\n",
        "    posterior = {x: 0.0 for x in pomdp.state_space}\n",
        "    for s in belief:\n",
        "        posterior[s] = belief.p(s) * pomdp.get_observation_distribution(s, action).p(observation)\n",
        "    return DiscreteDistribution(posterior).renormalize()\n",
        "\n",
        "\n",
        "def belief_filter(pomdp, belief, action, observation):\n",
        "    \"\"\"Compute the updated belief over the states based on the current action and obervation.\n",
        "\n",
        "    Specifically, the process is:\n",
        "        1. the agent is at state s, and has a belief about its current state p(s).\n",
        "        2. the agent takes an action a, and has a belief about its next state p(s' | a),\n",
        "            computed by transition_update.\n",
        "        3. the agent observes o, which follows the observation model of the POMDP p(o | s', a).\n",
        "        4. the agent updates its belief over the next state p(s' | o, a), following the Bayes rule.\n",
        "\n",
        "    Args:\n",
        "        pomdp (POMDP): A POMDP object.\n",
        "        belief (DiscreteDistribution): The belief about the agent's current state.\n",
        "        action: The action taken.\n",
        "        observation: The observation.\n",
        "\n",
        "    Returns:\n",
        "        next_belief: The belief about the next state by taking into consideration the action\n",
        "            at this step and the observation.\n",
        "    \"\"\"\n",
        "    return observation_update(pomdp, transition_update(pomdp, belief, action),\n",
        "                              action=action, observation=observation)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4f6815f",
      "metadata": {
        "id": "c4f6815f"
      },
      "source": [
        "## Belief-Space MDP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d9b3041",
      "metadata": {
        "id": "6d9b3041"
      },
      "source": [
        "### Question\n",
        "In this section, you will implement a function `create_belief_mdp`, that transforms a POMDP into a belief-space MDP.\n",
        "    We have provided the basic skeleton for you. In particular, you only need to implement the get_reward and the get_transition_distribution\n",
        "    function for the Belief MDP.  You can use the function `belief_filter(pomdp, b, a, o)` which is already defined; the implementaation of that function is available in the colab.\n",
        "\n",
        "For reference, our solution is **81** line(s) of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b92c8615",
      "metadata": {
        "id": "b92c8615"
      },
      "outputs": [],
      "source": [
        "def create_belief_mdp(pomdp):\n",
        "    \"\"\"Constructs a belief-space MDP from a POMDP.\n",
        "\n",
        "    Args:\n",
        "        pomdp: The input POMDP object.\n",
        "\n",
        "    Returns:\n",
        "        belief_mdp: The constructed belief-space MDP.\n",
        "    \"\"\"\n",
        "    def state_is_terminal(belief):\n",
        "        \"\"\"The state_is_terminal function for the belief-space MDP. It returns true iff. all possible states\n",
        "        in the belief are terminal states.\n",
        "\n",
        "        Args:\n",
        "            belief: A DiscreteDistribution of the state.\n",
        "\n",
        "        Returns:\n",
        "            is_terminal: Whether the current belief is a \"terminal\" belief.\n",
        "        \"\"\"\n",
        "        for state, p in belief.items():\n",
        "            if p > 0 and not pomdp.state_is_terminal(state):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def get_reward(belief, action, next_belief=None):\n",
        "        \"\"\"Compute the expected reward function for the belief-space MDP.\n",
        "\n",
        "        You only need to implement the case where the original reward function only\n",
        "        depends on the state and the action (but not the next state).\n",
        "\n",
        "        In this case, the reward function of the belief-space MDP will be only a function\n",
        "        of belief and action, but not next_belief.\n",
        "\n",
        "        In general (where the reward function if a function of state, action, and next_action),\n",
        "        in order to compute the expected reward, we need to also marginalize over the next state\n",
        "        distribution (which is next_belief).\n",
        "\n",
        "        Args:\n",
        "            belief: A DiscreteDistribution of the state.\n",
        "            action: An action.\n",
        "            next_belief: A DiscreteDistribution of the next state. Should be ignored (see above).\n",
        "\n",
        "        Returns:\n",
        "            reward: the expected reward at this step.\n",
        "        \"\"\"\n",
        "        r = 0.0\n",
        "        for s, p_s in belief.items():\n",
        "            # next_state is not needed by RobotChargingPOMDP, so we can pass None\n",
        "            r += p_s * pomdp.get_reward(s, action, next_state=None)\n",
        "        return r\n",
        "\n",
        "    def get_transition_distribution(belief, action):\n",
        "        \"\"\"Compute the transition distribution for an input belief and an action.\n",
        "\n",
        "        Specifically, the output will be a distribution over beliefs. That is, a distribution over\n",
        "        distributions. Since we have restricted our observation space to be finite, the\n",
        "        possible next belief is also a finite space. Thus, we can still use a DiscreteDistribution\n",
        "        object to represent the distribution over the next belief.\n",
        "\n",
        "        Args:\n",
        "            belief: A DiscreteDistribution of the state.\n",
        "            action: An action.\n",
        "\n",
        "        Returns:\n",
        "            next_belief: A DiscreteDistribution of the next state.\n",
        "        \"\"\"\n",
        "        from collections import defaultdict\n",
        "        dist_over_next_beliefs = defaultdict(float)\n",
        "\n",
        "        # If this belief is already terminal (absorbing), it stays put:\n",
        "        if state_is_terminal(belief):\n",
        "            # A one-hot distribution on itself (no next beliefs).\n",
        "            return OnehotDiscreteDistribution(belief)\n",
        "\n",
        "        # 1) First compute the \"prior\" over next states p(s') = sum_s b(s) * P(s->s'|a).\n",
        "        prior = transition_update(pomdp, belief, action)\n",
        "\n",
        "        # 2) For each observation o, compute p(o | b, a) and the posterior next belief\n",
        "        for o in pomdp.observation_space:\n",
        "            # p(o | b, a) = sum_{s'} p(s') * p(o | s', a)\n",
        "            p_o = 0.0\n",
        "            for s_prime in prior:\n",
        "                p_o += prior.p(s_prime) * pomdp.get_observation_distribution(s_prime, action).p(o)\n",
        "\n",
        "            if p_o > 0:\n",
        "                # The next belief distribution after observing o\n",
        "                next_b = observation_update(pomdp, prior, action, o)\n",
        "                dist_over_next_beliefs[next_b] += p_o\n",
        "\n",
        "        # Renormalize the distribution over next beliefs\n",
        "        return DiscreteDistribution(dist_over_next_beliefs).renormalize()\n",
        "\n",
        "\n",
        "    # Construct a new MDP based on the functions defined above.\n",
        "    return LambdaMDP(\n",
        "        state_space=None,  # We are not going to specify the state space explicitly (it's a continuous space).\n",
        "        action_space=pomdp.action_space,\n",
        "        state_is_terminal_fn=state_is_terminal,\n",
        "        get_reward_fn=get_reward,\n",
        "        get_transition_distribution_fn=get_transition_distribution,\n",
        "        temporal_discount_factor=pomdp.temporal_discount_factor\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d783ec49",
      "metadata": {
        "id": "d783ec49"
      },
      "source": [
        "### Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bc87b5bd",
      "metadata": {
        "id": "bc87b5bd",
        "outputId": "b0da0d1a-7e13-4347-d253-86ef90434c81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tests passed.\n"
          ]
        }
      ],
      "source": [
        "def test1_create_belief_mdp():\n",
        "    pomdp = RobotChargingPOMDP()\n",
        "    belief_mdp = create_belief_mdp(pomdp)\n",
        "    b4 = DiscreteDistribution({0 : .4, 1: .3, 2: .3})\n",
        "    a, Q = expectimax_search(b4, belief_mdp, 4, return_Q=True)\n",
        "\n",
        "    assert a == 'l1'\n",
        "    gt = {'l0': -0.1, 'l1': 0.255914, 'l2': -0.1, 'm12': -0.471128, 'm01': -0.5, 'm20': -0.031586, 'c': -56.0, 'nop': 0.0}\n",
        "    for k, v in gt.items():\n",
        "        assert k in Q and np.allclose(v, Q[k])\n",
        "\n",
        "test1_create_belief_mdp()\n",
        "\n",
        "print('Tests passed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13b9c199",
      "metadata": {
        "id": "13b9c199"
      },
      "source": [
        "## Receding Horizon Control\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f5dab55",
      "metadata": {
        "id": "2f5dab55"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a76399a",
      "metadata": {
        "id": "4a76399a"
      },
      "source": [
        "A simple implementation of the Receding Horizon Control (RHC).\n",
        "**Note**: these imports and functions are available in catsoop. You do not need to copy them in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a0564a8",
      "metadata": {
        "id": "9a0564a8"
      },
      "outputs": [],
      "source": [
        "def receding_horizon_control(pomdp, h=4, search_algo=expectimax_search):\n",
        "    \"\"\"Receding Horizon Control (RHC).\n",
        "\n",
        "    Args:\n",
        "        pomdp (POMDP): The input POMDP problem.\n",
        "        h (int): The receding horizon.\n",
        "\n",
        "    Returns:\n",
        "        policy (Callable): A function policy(belief) -> action, mapping from a belief state to the action to take.\n",
        "    \"\"\"\n",
        "\n",
        "    belief_mdp = create_belief_mdp(pomdp)\n",
        "\n",
        "    def policy(belief):\n",
        "        \"\"\"The RHC policy. Basically, it runs a search algorithm (e.g., expectimax_search)\n",
        "            with a fixed horizon and output the optimal policy at the current belief.\n",
        "\n",
        "        Args:\n",
        "            belief (DiscreteDistribution): The current belief.\n",
        "\n",
        "        Returns:\n",
        "            action: The next action to take.\n",
        "        \"\"\"\n",
        "\n",
        "        return search_algo(belief, belief_mdp, horizon=h)\n",
        "\n",
        "    return policy\n",
        "\n",
        "\n",
        "def simulate(pomdp, initial_belief, policy, n=4, real_s=None):\n",
        "    \"\"\"Simulate a policy on a POMDP.\n",
        "\n",
        "    Specifically, the function commands the environment for n timesteps.\n",
        "    We will keep track of two variables.\n",
        "        - robot_b, which is the current belief.\n",
        "        - real_s: the \"true\" state of the world.\n",
        "\n",
        "    Args:\n",
        "        pomdp (POMDP): The input POMDP problem.\n",
        "        initial_belief (DiscreteDistribution): a distribution of the state (the initial belief).\n",
        "        n (int): The nunber of simulation steps.\n",
        "        real_s: The initial real_s, can be None, in which case it will be sampled from initial_belief.\n",
        "        policy_gen (Callable): a function that maps a belief to the next action.\n",
        "    \"\"\"\n",
        "\n",
        "    import numpy.random as npr; npr.seed(0)  # to determinize the execution.\n",
        "\n",
        "    # Create the Belief MDP.\n",
        "    bmdp = create_belief_mdp(pomdp)\n",
        "\n",
        "    robot_b = initial_belief\n",
        "    if real_s is None:\n",
        "        real_s = robot_b.draw()\n",
        "\n",
        "    print('Robot belief:', robot_b)\n",
        "    print('Real state:', real_s)\n",
        "    print('')\n",
        "\n",
        "    for t in range(n):\n",
        "        print('Step', t)\n",
        "        # search_algo returns the optimal action at the current belief.\n",
        "        a = policy(robot_b)\n",
        "\n",
        "        print('  Executing:', a)\n",
        "        real_s = pomdp.get_transition_distribution(real_s, a).draw()\n",
        "        print('  Real State:', real_s)\n",
        "        if pomdp.state_is_terminal(real_s):\n",
        "            print('Terminated.')\n",
        "            break\n",
        "        o = pomdp.get_observation_distribution(real_s, a).draw()\n",
        "        print('  Observation:', o)\n",
        "        robot_b = belief_filter(pomdp, robot_b, a, o)\n",
        "        print('  Robot belief:', robot_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a819a4ed",
      "metadata": {
        "id": "a819a4ed"
      },
      "source": [
        "### Question\n",
        "Use the RHC implementation provided in the colab and answer the questions in Catsoop.  **Note that the simulation function sets the random seed so that your results will be deterministic.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d55508fa",
      "metadata": {
        "id": "d55508fa"
      },
      "source": [
        "You can run the following code to visualize the execution of your RHC-Expectimax policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23773bb8",
      "metadata": {
        "id": "23773bb8"
      },
      "outputs": [],
      "source": [
        "pomdp = RobotChargingPOMDP(gamma=0.9)\n",
        "b0 = DiscreteDistribution({0: 0.03, 1: 0.07, 2: 0.9})\n",
        "policy = receding_horizon_control(pomdp, 4, search_algo=expectimax_search)\n",
        "simulate(pomdp, b0, policy=policy, n=10, real_s=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c18c3cd",
      "metadata": {
        "id": "3c18c3cd"
      },
      "source": [
        "## Receding Horizon Control with Most-Likely-Observation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddb301e7",
      "metadata": {
        "id": "ddb301e7"
      },
      "source": [
        "### Question\n",
        "Complete the RHC-MLO implementation provided in the colab and answer the questions in Catsoop.\n",
        "The only new requirement is to implement the `expectimax_search_mlo` that instead of computing the\n",
        "expected reward over all possible observations, only focuses on the most-likely observation.<br />\n",
        "**HINT**: You only need very MINIMAL changes to the original `expectimax_search` code.<br />\n",
        "**HINT**: You may find the `DiscreteDistribution.max()` function useful.\n",
        "\n",
        "After implementing `expectimax_search_mlo`, use the following code snippets (in colab) to simulate RHC-MLO, and answer questions in Catsoop.\n",
        "Specifically, we will use `gamma = 0.9` and initial belief $b = (0.4, 0.3, 0.3)$ (the same one we used in the Most Likely State problem).  Note that we are specifying the actual initial state to be 2.\n",
        "\n",
        "\n",
        "For reference, our solution is **21** line(s) of code.\n",
        "\n",
        "In addition to all the utilities defined at the top of the Colab notebook, the following functions are available in this question environment: `create_belief_mdp`. You may not need to use all of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f6180af",
      "metadata": {
        "id": "5f6180af"
      },
      "outputs": [],
      "source": [
        "def expectimax_search_mlo(initial_state, belief_mdp, horizon, return_Q=False):\n",
        "    \"\"\"Use expectimax search to determine a next action.\n",
        "\n",
        "    Note that we're just computing the single next action to\n",
        "    take, we do not need to store the entire partial V.\n",
        "\n",
        "    Horizon is given as a separate argument so that we can use\n",
        "    expectimax search with receding horizon control, for example,\n",
        "    even if belief_mdp.horizon is inf.\n",
        "\n",
        "    Args:\n",
        "        initial_state: A state in the belief_mdp.\n",
        "        belief_mdp: An MDP.\n",
        "        horizon: An int horizon.\n",
        "        return_Q: A boolean value. If true, also return the Q value\n",
        "            at the root instead of the action.\n",
        "\n",
        "    Returns:\n",
        "        action: An action in the belief_mdp.\n",
        "        Q: The Q value at the root state (only when return_Q is True).\n",
        "    \"\"\"\n",
        "    A = sorted(belief_mdp.action_space)\n",
        "    R = belief_mdp.get_reward\n",
        "    P = belief_mdp.get_transition_distribution\n",
        "    gm = belief_mdp.temporal_discount_factor\n",
        "    ts = belief_mdp.state_is_terminal\n",
        "\n",
        "    def V(s, h):\n",
        "        if h == horizon or ts(s):\n",
        "            return 0\n",
        "        return max(Q(s, a, h) for a in A)\n",
        "\n",
        "    def Q(s, a, h):\n",
        "        # TODO: Your code here.\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    Q_values = {a: Q(initial_state, a, 0) for a in A}\n",
        "    if return_Q:\n",
        "        return max(A, key=Q_values.get), Q_values\n",
        "    return max(A, key=Q_values.get)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d17d1611",
      "metadata": {
        "id": "d17d1611"
      },
      "source": [
        "### Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48579f99",
      "metadata": {
        "id": "48579f99"
      },
      "outputs": [],
      "source": [
        "def test1_mlo():\n",
        "    pomdp = RobotChargingPOMDP(gamma=0.9)\n",
        "    policy = receding_horizon_control(pomdp, 4, search_algo=expectimax_search_mlo)\n",
        "\n",
        "    b0 = DiscreteDistribution({0: 0.4, 1: 0.3, 2: 0.3})\n",
        "    assert policy(b0) == 'm20'\n",
        "    b1 = DiscreteDistribution({0: 0.64, 1: 0.3, 2: 0.05999999999999998, 'T': 0.0})\n",
        "    assert policy(b1) == 'l0'\n",
        "    b2 = DiscreteDistribution({0: 0.8, 1: 0.16666666666666666, 2: 0.03333333333333331})\n",
        "    assert policy(b2) == 'l1'\n",
        "\n",
        "test1_mlo()\n",
        "\n",
        "print('Tests passed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4336963",
      "metadata": {
        "id": "f4336963"
      },
      "source": [
        "You can run the following code to visualize the execution of your RHC-Expectimax-MLO policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5143efda",
      "metadata": {
        "id": "5143efda"
      },
      "outputs": [],
      "source": [
        "pomdp = RobotChargingPOMDP(gamma=0.9)\n",
        "b0 = DiscreteDistribution({0: 0.4, 1: 0.3, 2: 0.3})\n",
        "policy = receding_horizon_control(pomdp, 4, search_algo=expectimax_search_mlo)\n",
        "simulate(pomdp, b0, policy=policy, n=10, real_s=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2213d6d5",
      "metadata": {
        "id": "2213d6d5"
      },
      "source": [
        "## QMDP (Optional)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d1db797",
      "metadata": {
        "id": "7d1db797"
      },
      "source": [
        "### Question\n",
        "Complete the QMDP implementation provided in the colab and answer the questions in catsoop. Here you can use the value function\n",
        "computed by the `value_iteration` algorithm provided and focus on computing the policy.\n",
        "\n",
        "For reference, our solution is **21** line(s) of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c3fcd20",
      "metadata": {
        "id": "3c3fcd20"
      },
      "outputs": [],
      "source": [
        "def qmdp(pomdp):\n",
        "    \"\"\"QMDP algorithm.\n",
        "    This function takes a POMDP as input and output a policy function that maps belief to the action to take.\n",
        "\n",
        "    Args:\n",
        "        pomdp (POMDP): An POMDP.\n",
        "\n",
        "    Returns:\n",
        "        policy (Callable): A function policy(belief) -> action, mapping from a belief state to the action to take.\n",
        "    \"\"\"\n",
        "    V, _ = value_iteration(pomdp)  # run value iteration on the underlying MDP.\n",
        "    Q = qsa_from_vs(pomdp, V)   # constructs Q values from the value function.\n",
        "\n",
        "    def policy(belief):\n",
        "        \"\"\"The QMDP policy.\n",
        "\n",
        "        Args:\n",
        "            belief (DiscreteDistribution): The belief about the current state.\n",
        "\n",
        "        Returns:\n",
        "            action: The argmax action computed based on QMDP.\n",
        "        \"\"\"\n",
        "        # TODO: Your code here.\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    return policy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0181ab1d",
      "metadata": {
        "id": "0181ab1d"
      },
      "source": [
        "### Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa92c16a",
      "metadata": {
        "id": "aa92c16a"
      },
      "outputs": [],
      "source": [
        "def test1_qmdp():\n",
        "    pomdp = RobotChargingPOMDP()\n",
        "    policy = qmdp(pomdp)\n",
        "    b0 = DiscreteDistribution({0: 0.999, 1: 0.001, 2: 0.0})\n",
        "    assert policy(b0) == 'c'\n",
        "    b0 = DiscreteDistribution({0: 0.0, 1: 0.6, 2: 0.4})\n",
        "    assert policy(b0) == 'm12'\n",
        "    b0 = DiscreteDistribution({0: 0.1, 1: 0.33, 2: 0.57})\n",
        "    assert policy(b0) == 'm20'\n",
        "\n",
        "test1_qmdp()\n",
        "\n",
        "print('Tests passed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd68867d",
      "metadata": {
        "id": "bd68867d"
      },
      "source": [
        "You can run the following code to visualize the execution of your QMDP policy (Hint: QMDP is not a good strategy for this case)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a49e53f",
      "metadata": {
        "id": "3a49e53f"
      },
      "outputs": [],
      "source": [
        "pomdp = RobotChargingPOMDP(gamma=0.9)\n",
        "b0 = DiscreteDistribution({0: 0.03, 1: 0.07, 2: 0.9})\n",
        "policy = qmdp(pomdp)\n",
        "simulate(pomdp, b0, policy=policy, n=10, real_s=2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "hw09.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}